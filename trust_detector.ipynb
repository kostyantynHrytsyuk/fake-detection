{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-1bfacfb553ec>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-1bfacfb553ec>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    import keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Reshape, Concatenate, LeakyReLU\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from math import floor\n",
    "from os.path import isfile, join\n",
    "from scipy.ndimage.interpolation import zoom, rotate\n",
    "\n",
    "\n",
    "import face_recognition\n",
    "import imageio\n",
    "\n",
    "import keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Reshape, Concatenate, LeakyReLU\n",
    "import keras.models import Model\n",
    "import keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining MesoNet CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFModel:\n",
    "    def __init__(self, lr):\n",
    "        self.model = self._configure_model()\n",
    "        optimizer = Adam(lr = lr)\n",
    "        self.model.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['accuracy'])\n",
    "\n",
    "    def _configure_model(self):\n",
    "        # Input layer\n",
    "        # Height, Width, # of channels\n",
    "        X = Input(shape = (256, 256, 3))\n",
    "\n",
    "        # Each batch below - is convolutional block\n",
    "        \n",
    "        # It is a convolutional layer\n",
    "        # First parameter is a dimensionality of output space\n",
    "        # Second parameter is a kernel size - size of convolutional window\n",
    "        # Parameter 'padding' sets whether the dimensionalty of input is the same as of output\n",
    "        # Parameter 'activation' sets the type of activation function\n",
    "        # Double parentheses syntax - is specific of Keras model initialization\n",
    "        X_1 = Conv2D(8, (3,3), padding='same', activation='relu')(x)\n",
    "        # This layer does a data normalization:\n",
    "        # Transform data in such a way that mean is near 0 and sd is near 1\n",
    "        X_1 = BatchNormalization()(X_1)\n",
    "        # This layer reduces dimensionality of output by taking the max value out of each \"pool\"\n",
    "        # Parameter pool_size define the size of pool\n",
    "        X_1 = MaxPooling2D(pool_size=(2,2), padding='same')(X_1)\n",
    "\n",
    "        # The same explanation as below is applicable to each code batch below\n",
    "        X_2 = Conv2D(8, (5,5), padding='same', activation='relu')(X_1)\n",
    "        X_2 = BatchNormalization()(X_2)\n",
    "        X_2 = MaxPooling2D(pool_size=(2,2), padding='same')(X_2)\n",
    "\n",
    "\n",
    "        X_3 = Conv2D(16, (5,5), padding='same', activation='relu')(X_2)\n",
    "        X_3 = BatchNormalization()(X_3)\n",
    "        X_3 = MaxPooling2D(pool_size=(2,2), padding='same')(X_3)\n",
    "\n",
    "        X_4 = Conv2D(16, (5,5), padding='same', activation='relu')(X_3)\n",
    "        X_4 = BatchNormalization()(X_4)\n",
    "        X_4 = MaxPooling2D(pool_size=(2,2), padding='same')(X_4)\n",
    "\n",
    "        y = Flatten()(x4)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(16)(y)\n",
    "        y = LeakyReLu(alpha=0.1)(y)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(1, activation='sigmoid')(y)\n",
    "\n",
    "        return Model(inputs=x, outputs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To avoid processing all video frame, we can reduce it only to the face area.\n",
    "\n",
    "## Here we introduce the wrappers for entites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoWrapper:\n",
    "    def __init__(self, path):\n",
    "        self.container = imageio.get_reader(path, 'ffmpeg')\n",
    "        self.length = self.container.count_frames()\n",
    "        self.fps = self.container.get_meta_data()['fps']\n",
    "        self.path = path\n",
    "        \n",
    "    def __call__(self, key):\n",
    "        return self.get(key)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def init_head(self):\n",
    "        self.container.set_image_index(0)\n",
    "    \n",
    "    def next_frame(self):\n",
    "        self.container.get_next_data()\n",
    "    \n",
    "    def get(self, key):\n",
    "        return self.container.get_data(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceWrapper(VideoWrapper):\n",
    "    def __init__(self, path, first_face = True):\n",
    "        super().__init__(path)\n",
    "        self.face_locations = {}\n",
    "        # Each face has such coords: (center, length, rotation)\n",
    "        self.coordinates = {}\n",
    "        \n",
    "        self.last_frame = self.get(0)\n",
    "        self.frame_shape = self.last_frame.shape[:2]\n",
    "        self.last_location = (0, 200, 200, 0)\n",
    "        \n",
    "        if first_face:\n",
    "            face_positions = face_recognition.face_locations(self.last_frame, sample_freq = 2)\n",
    "            if len(face_positions) > 0:\n",
    "                self.last_location = face_positions[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def pop_largest_location(coords_list):\n",
    "        max_location = coords_list[0]\n",
    "        max_size = 0\n",
    "        if len(coords_list) > 1:\n",
    "            for coord in coords_list:\n",
    "                size = coord[2] - coord[0]\n",
    "                if size > max_size:\n",
    "                    max_size = size\n",
    "                    max_location = coord\n",
    "        return max_location\n",
    "    \n",
    "    @staticmethod\n",
    "    def upsample_location(reduced_location, upsampled_origin, factor):\n",
    "        y0, x1, y1, x0 = reduced_location\n",
    "        Y0 = np.round(upsampled_origin[0] + y0 * factor)\n",
    "        X1 = np.round(upsampled_origin[1] + x1 * factor)\n",
    "        Y1 = np.round(upsampled_origin[0] + y1 * factor)\n",
    "        X0 = np.round(upsampled_origin[1] + x0 * factor)\n",
    "        return (Y0, X1, Y1, X0)\n",
    "    \n",
    "    def expand_location_zone(self, coords, margin = 0.2):\n",
    "        # Expanding face area by the margin value\n",
    "        offset = np.round(margin * (loc[2] - loc[0]))\n",
    "        y0 = np.max(loc[0] - offset, 0)\n",
    "        x1 = np.min(loc[1] + offset, self.frame_shape[1])\n",
    "        y1 = np.min(loc[2] + offset, self.frame_shape[0])\n",
    "        x0 = np.max(loc[3] - offset, 0)\n",
    "        return (y0, x1, y1, x0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def L2(A, B):\n",
    "        return np.sqrt(np.sum(np.square(A - B)))\n",
    "    \n",
    "    def get_face(self, f):\n",
    "        frame = self.get(f)\n",
    "        if i in self.faces:\n",
    "            loc = self.faces[i]\n",
    "            patch = frame[loc[0]:loc[2], loc[3]:loc[1]]\n",
    "            \n",
    "            return patch\n",
    "        return frame\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_image_slice(img, y0, y1, x0, x1):\n",
    "        m, n = img.shape[:2]\n",
    "        padding = max(-y0, y1-m, -x0, x1-n, 0)\n",
    "        padded_img = np.pad(img, ((padding, padding), (padding, padding), (0, 0)), 'reflect')\n",
    "        return padded_img[(padding + y0):(padding + y1),\n",
    "                        (padding + x0):(padding + x1)]\n",
    "    \n",
    "    def find_coordinates(self, landmark, K=2.2):\n",
    "        # Process face features\n",
    "        eye_1 = np.mean(landmark['left_eye'], axis=0)\n",
    "        eye_2 = np.mean(landmark['right_eye'], axis=0)\n",
    "        \n",
    "        eyes = (eye_1 + eye_2) / 2\n",
    "        \n",
    "        nose = np.mean(landmark['nose_tip'], axis=0) / 2 + np.mean(landmark['nose_bridge'], axis=0) / 2\n",
    "        \n",
    "        top_lip = np.mean(landmark['top_lip'], axis=0)\n",
    "        bot_lip = np.mean(landmark['bottom_lip'], axis=0)\n",
    "        lips = (top_lip + bot_lip) / 2\n",
    "        \n",
    "        c = nose\n",
    "        eyes_l2 = self.L2(eye_1, eye_2)\n",
    "        face_l2 = self.L2(lips, eyes)\n",
    "        \n",
    "        center = np.max(eyes_l2, face_l2) * K\n",
    "        \n",
    "        if lips[1] == eyes[1]:\n",
    "            if B[0] > E[0]:\n",
    "                rot = 90\n",
    "            else:\n",
    "                rot = -90\n",
    "        else:\n",
    "            rot = np.arctan((lips[0] - eyes[0]) / (lips[1] - eyes[1])) / np.pi * 180\n",
    "                 # Center                              length           rotation\n",
    "        return ( (floor(center[1]), floor(center[0])), floor(distance), rot)\n",
    "    \n",
    "    # Main function in face processing    \n",
    "    def localize_face(self, resize = 0.5, stop = 0, skipstep = 0, no_face_acceleration_threshold = 3, cut_left = 0, cut_right = -1, use_frameset = False, frameset = []):\n",
    "        not_found = 0\n",
    "        no_face = 0\n",
    "        # Face acceleration\n",
    "        no_face_acc = 0\n",
    "        \n",
    "        # Using predefined frameset\n",
    "        if (use_frameset):\n",
    "            finder_frameset = frameset\n",
    "        else:\n",
    "            if (stop != 0):\n",
    "                finder_frameset = range(0, min(self.length, stop), skipstep + 1)\n",
    "            else:\n",
    "                finder_frameset = range(0, self.length, skipstep + 1)\n",
    "        \n",
    "        for i in finder_frameset:\n",
    "            frame = self.get(i)\n",
    "            # Cutting face out of frame\n",
    "            if (cut_left != 0 or cut_right != -1):\n",
    "                frame[:, :cut_left] = 0\n",
    "                frame[:, cut_right:] = 0\n",
    "                \n",
    "            # Start looking in the area from previous step\n",
    "            potential_location = self.expand_location_zone(self.last_location)\n",
    "            potential_face_patch = frame[potential_location[0]:potential_location[2], potential_location[3]:potential_location[1]]\n",
    "            # left upper corner\n",
    "            potential_face_patch_origin = (potential_location[0], potential_location[3])\n",
    "            \n",
    "            reduced_potential_face_patch = zoom(potential_face_patch, (resize, resize, 1))\n",
    "            reduced_face_locations = face_recognition.face_locations(reduced_potential_face_patch, model = 'cnn')\n",
    "            \n",
    "            # If face is found\n",
    "            if len(reduced_face_locations) > 0:\n",
    "                no_face_acc = 0\n",
    "\n",
    "                reduced_face_location = self.pop_largest_location(reduced_face_locations)\n",
    "                face_location = self.upsample_location(reduced_face_location, potential_face_patch_origin, 1 / resize)\n",
    "                \n",
    "                # Add new defined face\n",
    "                self.faces[i] = face_location\n",
    "                # Update last face location\n",
    "                self.last_location = face_location\n",
    "                \n",
    "                # Extract face coords (rotation, length and center) from landmarks\n",
    "                landmarks = face_recognition.face_landmarks(frame, [face_location])\n",
    "                \n",
    "                if len(landmarks) > 0:\n",
    "                    # we assume that there is one and only one landmark group\n",
    "                    self.coordinates[i] = self.find_coordinates(landmarks[0])\n",
    "            # If face is not found\n",
    "            else:\n",
    "                not_found += 1\n",
    "                \n",
    "                if no_face_acc < no_face_acceleration_threshold:\n",
    "                    # Look over frame for face \n",
    "                    face_locations = face_recognition.face_locations(frame, number_of_times_to_upsample = 2)\n",
    "                else:\n",
    "                    # Avoid spending to much time on a long scene without faces\n",
    "                    reduced_frame = zoom(frame, (resize, resize, 1))\n",
    "                    face_locations = face_recognition.face_locations(reduced_frame)\n",
    "                \n",
    "                if len(face_locations) > 0:\n",
    "                    no_face_acc = 0\n",
    "                    \n",
    "                    face_location = self.pop_largest_location(face_locations)\n",
    "                    \n",
    "                    # Upsample location\n",
    "                    if no_face_acc > no_face_acceleration_threshold:\n",
    "                        face_location = self.upsample_location(face_location, (0, 0), 1 / resize)\n",
    "                    \n",
    "                    # Add new defined face\n",
    "                    self.faces[i] = face_location\n",
    "                    # Update last face location\n",
    "                    self.last_location = face_location\n",
    "                    \n",
    "                    # Extract face coords (rotation, length and center) from landmarks\n",
    "                    landmarks = face_recognition.face_landmarks(frame, [face_location])\n",
    "                    if len(landmarks) > 0:\n",
    "                        self.coordinates[i] = self.find_coordinates(landmarks[0])\n",
    "                        \n",
    "                else:\n",
    "                    print('Face extraction warning : ',i, '- no face')\n",
    "                    no_face_acc += 1\n",
    "                    no_face += 1\n",
    "                \n",
    "        return 0\n",
    "    \n",
    "    def get_aligned_face(self, i, l_factor = 1.3):\n",
    "        \n",
    "        frame = self.get(i)\n",
    "        \n",
    "        if i in self.coordinates:\n",
    "            # center, length, rotation\n",
    "            c, l, r = self.coordinates[i]\n",
    "            l = int(l) * l_factor\n",
    "            dl_ = floor(np.sqrt(2) * l / 2)\n",
    "            patch = self.get_image_slice(frame,\n",
    "                                    floor(c[0] - dl_),\n",
    "                                    floor(c[0] + dl_),\n",
    "                                    floor(c[1] - dl_),\n",
    "                                    floor(c[1] + dl_))\n",
    "            rotated_patch = rotate(patch, -r, reshape=False)\n",
    "            # note : dl_ is the center of the patch of length 2dl_\n",
    "            return self.get_image_slice(rotated_patch,\n",
    "                                    floor(dl_-l//2),\n",
    "                                    floor(dl_+l//2),\n",
    "                                    floor(dl_-l//2),\n",
    "                                    floor(dl_+l//2))\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FaceBatchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model of subset of frame for video\n",
    "class FaceBatchGenerator:\n",
    "    \n",
    "    def __init__(self, face_finder, target_size = 256):\n",
    "        self.finder = face_finder\n",
    "        self.target_size = target_size\n",
    "        self.head = 0\n",
    "        self.length = int(face_finder.length)\n",
    "        \n",
    "    def resize_patch(self, patch):\n",
    "        m, n = patch.shape[:2]\n",
    "        return zoom(patch, (self.target_size / m, self.target_size / n, 1))\n",
    "    \n",
    "    def next_batch(self, batch_size=50):\n",
    "        batch = np.zeros((1, self.target_size, self.target_size, 3))\n",
    "        stop = np.min(self.head + batch_size, self.length)\n",
    "        i = 0\n",
    "        while (i < batch_size) and (self.head < self.length):\n",
    "            if self.head in self.finder.coordinates:\n",
    "                patch = self.finder.get_aligned_face(self.head)\n",
    "                batch = np.concatenate((batch, np.expand_dims(self.resize_patch(patch), axis = 0)),\n",
    "                                        axis = 0)\n",
    "                i += 1\n",
    "            self.head += 1\n",
    "        return batch[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_faces(generator, classifier, batch_size = 50, output_size = 1):\n",
    "    n = len(generator.finder.coordinates.items())\n",
    "    profile = np.zeros((1, output_size))\n",
    "    for epoch in range(n // batch_size + 1):\n",
    "        face_batch = generator.next_batch(batch_size = batch_size)\n",
    "        prediction = classifier.predict(face_batch)\n",
    "        if (len(prediction) > 0):\n",
    "            profile = np.concatenate((profile, prediction))\n",
    "    return profile[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model running, face extraction and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(classifier, dirname, frame_subsample_count = 30):\n",
    "    file_names = ''\n",
    "    predictions = {}\n",
    "    \n",
    "    for video_file in filenames:\n",
    "        path = join(dirname, vid)\n",
    "        face_finder = FaceWrapper(path, load_first_face = False)\n",
    "        step = max(floor(face_finder.length / frame_subsample_count), 0)\n",
    "        face_finder.find_faces(resize=0.5, skipstep = skipstep)\n",
    "        \n",
    "        gen = FaceBatchGenerator(face_finder)\n",
    "        pred = predict_faces(gen, classifier)\n",
    "        \n",
    "        predictions[vid[:-4]] = (np.mean(pred > 0.5), p)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "input_video = cv2.VideoCapture('video_1_raw.mp4')\n",
    "\n",
    "fps = int(input_video.get(cv2.CAP_PROP_FPS))\n",
    "frame_count = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(fps)\n",
    "print(frame_count)\n",
    "print(frame_width)\n",
    "print(frame_height)\n",
    "\n",
    "codec = cv2.VideoWriter.fourcc(*'XVID')\n",
    "video_writer = cv2.VideoWriter('video_1_processed.mp4', codec,fps, (frame_width, frame_height))\n",
    "\n",
    "face_locations = []\n",
    "\n",
    "count = 0\n",
    "percentage_of_frames = 4\n",
    "start = time.time()\n",
    "while (True):\n",
    "    ret, frame = input_video.read()\n",
    "    if count % percentage_of_frames == 0:\n",
    "        if not ret:\n",
    "            print(\"Video ended!\")\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        face_locations = face_recognition.face_locations(rgb_frame, model='cnn')\n",
    "\n",
    "        for top, right, bottom, left in face_locations:\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 225), 2)\n",
    "\n",
    "        video_writer.write(frame)\n",
    "\n",
    "        print('Processed ', count%percentage_of_frames, ' frames')\n",
    "\n",
    "    count += 1\n",
    "\n",
    "print('Result:', count)\n",
    "print('Taken time: ', (time.time() - start) % 60, ' minutes')\n",
    "\n",
    "input_video.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_38",
   "language": "python",
   "name": "python_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
